# 爬虫复习

[TOC]



## 开始一个爬虫

1、创建项目命令
scrapy startproject “项目名”
2、创建爬虫文件命令
scrapy genspider “爬虫名” “爬虫范围”
3、启动一个爬虫
scrapy crawl “爬虫名字”

## 代码编写流程

1、明确目标 编写items.py文件
2、构建爬虫 编写spiders 文件夹下面的爬虫文件 tz.py
3、存储数据 编写管道pipelines.py（记得在settings.py里面注册管道）

## yield在Spider中的作用

yield在函数中的作用：函数如果使用了yield它就会是一个生成器，scrapy会逐一获取parse方法中生成的结果，并判断结果是一个什么样的类型
1、yield scrapy.Request(url, callback) -->调度器入列
2、yield item  -->发送给管道文件

## scrapy框架

1. Scrapy Engine(**引擎**): 负责`Spider`、`ItemPipeline`、`Downloader`、`Scheduler`中间的通讯，信号、数据传递等。

2. Scheduler(**调度器**): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行**整理排列，入队**，当引擎需要时，交还给引擎。
3. Downloader（**下载器**）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，
4. Spider（**爬虫**）：它负责处理所有Responses,从中**分析提取数据**，获取Item字段需要的数据，并**将需要跟进的URL提交给引擎**，再次进入Scheduler(调度器)，
5. Item Pipeline(**管道**)：它负责处理Spider中获取到的Item，并进行进行**后期处理**（详细分析、过滤、存储等）的地方.
6. Downloader Middlewares（**下载中间件**）：你可以当作是一个可以自定义扩展下载功能的组件。
7. Spider Middlewares（**Spider中间件**）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）

## scrapy框架流程：

start_urls快速走了一边流程再开始parse()

1. 引擎拿到request请求发送给调度器整理入列
2. 调度器返回request请求给引擎
3. 引擎发送request请求给下载器
4. 下载器前往互联网进行下载response返回给引擎
5. 引擎把response返回给爬虫
6. 爬虫返回items或者request给引擎
7. items返回给管道，request进行下一轮循环



## Selectors选择器(scrapy内置)

Selector有四个基本的方法，

1. xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的`selector list`列表
2. extract(): 序列化该节点为Unicode字符串并返回list,extract_first防止使用[0]报错
3. css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表，语法同 BeautifulSoup4
4. re(): 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表



## spider文件的属性和方法

### Name:
定义spider名字的字符串。
例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite

### allowed_domains:
包含了spider允许爬取的域名(domain)的列表，可选。

### start_urls:
初始URL元祖/列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。

### start_requests(self):
该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取（默认实现是使用 start_urls 的url）的第一个Request。
当spider启动爬取并且未指定start_urls时，该方法被调用。

### parse(self, response):
当请求url返回网页没有指定回调函数时，默认的Request对象回调函数。用来处理网页返回的response，以及生成Item或者Request对象。

## scrapy shell

只有在scrapy项目中才可以使用

```python
scrapy shell "url"
```
```python
例如
scrapy shell baidu.com
response.xpath('//*[@id="lg"]/img/@src').extract_first()
返回：'//www.baidu.com/img/bd_logo1.png'
```



## crawlSpider

定义了一些规则(rule)来提供跟进link的方便的机制

CrawlSpider是Spider的派生类，
Spider类的设计原则是只爬取start_url列表中的网页，
而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，
从爬取的网页中获取link并继续爬取的工作更适合。

### 创建一个crawlspider文件：

```python
Scrapy genspider –t crawl “spider_name” “url”
```

### rules

#### link_extractor

是一个Link Extractor对象，用于定义需要提取的链接。
####callback

从link_extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 **parse**方法，crawl spider将会运行失败。

####follow

是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。

####process_links

指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。

#### process_request

指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request)

### LinkExtractors

```python
class scrapy.linkextractors.LinkExtractor(
    allow = (),
    deny = (),
    allow_domains = (),
    deny_domains = (),
    deny_extensions = None,
    restrict_xpaths = (),
    tags = ('a','area'),
    attrs = ('href'),
    canonicalize = True,
    unique = True,
    process_value = None
)
```

主要参数：

#### allow

满足括号中“正则表达式”的URL会被提取，如果为空，则全部匹配。

#### deny

满足括号中“正则表达式”的URL**一定不提取**（优先级高于allow）。

####allow_domains

会被提取的链接的domains。

####deny_domains

一定不会被提取链接的domains。

####restrict_xpaths

使用xpath表达式，和allow共同作用过滤链接。

## 使用logging设置

###Log levels
Scrapy提供5层logging级别:

- CRITICAL - 严重错误(critical)

- ERROR - 一般错误(regular errors)

- WARNING - 警告信息(warning messages)

- INFO - 一般信息(informational messages)

- DEBUG - 调试信息(debugging messages)

###logging设置

  通过在setting.py中进行以下设置可以被用来配置logging:

- LOG_ENABLED 默认: True，启用logging

- LOG_ENCODING 默认: 'utf-8'，logging使用的编码

- LOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名

- LOG_LEVEL 默认: 'DEBUG'，log的最低级别

- LOG_STDOUT 默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print "hello" ，其将会在Scrapy log中显示。

```python
开发工作中经常会加上以下两行：
LOG_FILE = “文件名.log"
LOG_LEVEL = "INFO"
```



