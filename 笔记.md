# 爬虫复习

[TOC]



## 开始一个爬虫

1. 创建项目命令：scrapy startproject “项目名”
2. 创建爬虫文件命令：scrapy genspider “爬虫名” “爬虫范围”
3. 启动一个爬虫：scrapy crawl “爬虫名字”

## 代码编写流程

1. 明确目标 编写items.py文件

2. 构建爬虫 编写spiders 文件夹下面的爬虫文件 tz.py
3. 存储数据 编写管道pipelines.py（记得在settings.py里面注册管道）

## yield在Spider中的作用

yield在函数中的作用：函数如果使用了yield它就会是一个生成器，scrapy会逐一获取parse方法中生成的结果，并判断结果是一个什么样的类型

1. yield scrapy.Request(url, callback) -->调度器入列
2. yield item  -->发送给管道文件

## scrapy框架

1. Scrapy Engine(**引擎**): 负责`Spider`、`ItemPipeline`、`Downloader`、`Scheduler`中间的通讯，信号、数据传递等。

2. Scheduler(**调度器**): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行**整理排列，入队**，当引擎需要时，交还给引擎。
3. Downloader（**下载器**）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，
4. Spider（**爬虫**）：它负责处理所有Responses,从中**分析提取数据**，获取Item字段需要的数据，并**将需要跟进的URL提交给引擎**，再次进入Scheduler(调度器)，
5. Item Pipeline(**管道**)：它负责处理Spider中获取到的Item，并进行进行**后期处理**（详细分析、过滤、存储等）的地方.
6. Downloader Middlewares（**下载中间件**）：你可以当作是一个可以自定义扩展下载功能的组件。
7. Spider Middlewares（**Spider中间件**）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）

## scrapy框架流程：

start_urls快速走了一边流程再开始parse()

1. 引擎拿到request请求发送给调度器整理入列
2. 调度器返回request请求给引擎
3. 引擎发送request请求给下载器
4. 下载器前往互联网进行下载response返回给引擎
5. 引擎把response返回给爬虫
6. 爬虫返回items或者request给引擎
7. items返回给管道，request进行下一轮循环



## Selectors选择器(scrapy内置)

Selector有四个基本的方法，

1. xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的`selector list`列表
2. extract(): 序列化该节点为Unicode字符串并返回list,extract_first防止使用[0]报错
3. css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表，语法同 BeautifulSoup4
4. re(): 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表



## spider文件的属性和方法

### Name:
定义spider名字的字符串。
例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite

### allowed_domains:
包含了spider允许爬取的域名(domain)的列表，可选。

### start_urls:
初始URL元祖/列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。

### start_requests(self):
该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取（默认实现是使用 start_urls 的url）的第一个Request。
当spider启动爬取并且未指定start_urls时，该方法被调用。

### parse(self, response):
当请求url返回网页没有指定回调函数时，默认的Request对象回调函数。用来处理网页返回的response，以及生成Item或者Request对象。

## scrapy shell

只有在scrapy项目中才可以使用

```python
scrapy shell "url"
```
```python
例如
scrapy shell baidu.com
response.xpath('//*[@id="lg"]/img/@src').extract_first()
返回:'//www.baidu.com/img/bd_logo1.png'
```



## crawlSpider

定义了一些规则(rule)来提供跟进link的方便的机制

CrawlSpider是Spider的派生类，
Spider类的设计原则是只爬取start_url列表中的网页，
而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，
从爬取的网页中获取link并继续爬取的工作更适合。

### 创建一个crawlspider文件：

```python
Scrapy genspider –t crawl “spider_name” “url”
```

### rules

#### link_extractor

是一个Link Extractor对象，用于定义需要提取的链接。
#### callback

从link_extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 **parse**方法，crawl spider将会运行失败。

#### follow

是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。

#### process_links

指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。

#### process_request

指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request)

### LinkExtractors

```python
class scrapy.linkextractors.LinkExtractor(
    allow = (),
    deny = (),
    allow_domains = (),
    deny_domains = (),
    deny_extensions = None,
    restrict_xpaths = (),
    tags = ('a','area'),
    attrs = ('href'),
    canonicalize = True,
    unique = True,
    process_value = None
)
```

主要参数：

#### allow

满足括号中“正则表达式”的URL会被提取，如果为空，则全部匹配。

#### deny

满足括号中“正则表达式”的URL**一定不提取**（优先级高于allow）。

#### allow_domains

会被提取的链接的domains。

#### deny_domains

一定不会被提取链接的domains。

#### restrict_xpaths

使用xpath表达式，和allow共同作用过滤链接。

## 使用logging设置

### Log levels
Scrapy提供5层logging级别:

- CRITICAL - 严重错误(critical)

- ERROR - 一般错误(regular errors)

- WARNING - 警告信息(warning messages)

- INFO - 一般信息(informational messages)

- DEBUG - 调试信息(debugging messages)

### logging设置

  通过在setting.py中进行以下设置可以被用来配置logging:

- LOG_ENABLED 默认: True，启用logging

- LOG_ENCODING 默认: 'utf-8'，logging使用的编码

- LOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名

- LOG_LEVEL 默认: 'DEBUG'，log的最低级别

- LOG_STDOUT 默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print "hello" ，其将会在Scrapy log中显示。

```python
开发工作中经常会加上以下两行:
LOG_FILE = "文件名.log"
LOG_LEVEL = "INFO"
```



##parse()方法的工作机制

1. 因为使用的yield，而不是return。parse函数将会被当做一个**生成器**使用。scrapy会逐一获取parse方法中生成的结果，并判断该结果是一个什么样的类型；
2. 如果是request则加入爬取队列，如果是item类型则使用pipeline处理，其他类型则返回错误信息。
3. scrapy取到第一部分的request不会立马就去发送这个request，只是把这个request放到队列里，然后接着从生成器里获取；
4. 取尽第一部分的request，然后再获取第二部分的item，取到item了，就会放到对应的pipeline里处理；
5. parse()方法作为回调函数(callback)赋值给了Request，指定parse()方法来处理这些请求 scrapy.Request(url, callback=self.parse)
6. Request对象经过调度，执行生成 scrapy.http.response()的响应对象，并送回给parse()方法，直到调度器中没有Request（递归的思路）
7. 取尽之后，parse()工作结束，引擎再根据队列和pipelines中的内容去执行相应的操作；
8. 程序在取得各个页面的items前，会先处理完之前所有的request队列里的请求，然后再提取items。
9. 这一切的一切，Scrapy引擎和调度器将负责到底。



## redis数据库操作

### Str

#### 保存

设置键值：set key value
设置键值及过期时间，以秒为单位：setex key seconds value
查看过期时间：ttl key
设置多个键值：mset key1 value1 key2 value2
获取多个值：mget key1 key2
追加值：append key value

#### 获取

根据键获取值，如果不存在则返回nil：get key
根据多个键获取多个值：mget key1 key2

#### 键命令

查看键，参数支持正则表达式：keys pattern
查看所有键：keys *
判断键是否存在，如果存在返回1，不存在返回0：exists key1
查看键对应的value的类型：tyoe key
输出键及对应的值：del key1 key2

### list

#### 增加

在左侧插入数据:lpush key value1 value2
在左侧插入数据:rpush key value1 value2
从指定元素的前或后插入新元素:linsert key before或者after 现有元素 新元素

#### 获取

lrange key start stop

设置指定索引位置的元素值:lset key index value

#### 删除

将列表中count此出现的值为value的元素移除
count>0:从头忘尾移除
count<0:从尾往头移除
lrem key count value

### hash

#### 增加、修改

设置单个属性
hset key field value
设置多个属性
hmset key field1 value1 field2 value2 ...

#### 获取

获取指定键所有的属性
hkeys key
获取一个属性的值
hget key field
获取多个属性的值
hmget key field1 field2...
获取所有属性的值
hvals key

#### 删除:

删除整个hash键及值，使用del命令
删除属性，属性对应的值会被一起删除
hdel key field1 field2

### set

#### 增加

添加元素
sadd key member1 member2

#### 获取:

返回所有的元素
smembers key

#### 删除:

srem key

### zset 有序集合

#### 增加:

zadd key score1 member1 score2 member2

#### 获取:

zrange key start stop
返回score值在min和max之间的成员
zrangebyscore key min max
返回成员member的score值
zscore key member

#### 删除:

删除指定元素
zrem key member1 member2
删除权重在指定范围的元素
zremrangebyscore key min max

## Redis主从

 一个master可以拥有多个slave，一个slave又可以拥有多个slave

master是用来写数据的，slave用来读数据，据统计：网站的读写比率是10:1

通过主从配置可以实现读写分离

master和slave都是一个redis实例（redis服务）

### 配置

- 修改etc/redis/redis.conf文件
  - sudo vi redis.conf
  - bind 192.168.x.x

- 重启redis服务

  - sudo service redis stop
  - redis-server redis.conf


- 复制etc/redis/redis.conf文件
  - sudo cp redis.conf ./slave.conf

- 修改redis/slave.conf文件
  - sudo vi slave.conf

- 编辑内容
  - bind 192.168.xxx.xxx
  - slaveof 192.168.xxx.xxx 6379
  - port 6378

- redis服务
  - sudo redis-server slave.conf

- 查看主从关系
  - redis-cli -h 192.168.xxx.xxx info Replication

#### 参考阅读

redis集群搭建：http://www.cnblogs.com/wuxl360/p/5920330.html
[Python]搭建redis集群：http://blog.5ibc.net/p/51020.html